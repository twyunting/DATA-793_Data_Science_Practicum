# -*- coding: utf-8 -*-
"""03.construct_Xfeatures_ylabels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yjESHiLCM3wp17BIkALtiPUt434Ekh78

# Install the required packages
"""

#!cat /proc/meminfo

import cv2
import matplotlib.pyplot as plt
import matplotlib
from PIL import Image
import sys
import os
from matplotlib import pyplot
from matplotlib.patches import Rectangle
#import imageio
import glob
import numpy as np

# Ref: https://www.geeksforgeeks.org/python-os-chdir-method/
# Ref: https://stackoverflow.com/questions/9234560/find-all-csv-files-in-a-directory-using-python/12280052 
#os.chdir(r"C:\Users\Gfg\Desktop\geeks")
#print("Directory changed")

# This is the path where you want to search
#path = '/content/drive/MyDrive/American_University/2021_Fall/DATA-793-001_Data Science Practicum/Datasets/manipulated_sequences/Deepfakes/raw/videos'

# this is the extension you want to detect
extension = '.jpg'

def get_list_filenames(cwd, path, extension):
    try:
        os.chdir(path)
        print("Successfully change the directory!")
        filenames = glob.glob('*.{}'.format(extension))
        print("Number of files: ", len(filenames))

    # Caching the exception
    except: 
        print("Something wrong with specified directory. Exception- ", sys.exc_info()) 

    # handling with finally: restore the path which is the current directory before changing directory           
    finally: 
        print("Restoring the path") 
        os.chdir(cwd) # Change it back to cwd (original directory)
        print("Current directory is-", os.getcwd()) 
        
    return filenames


# Ref: https://realpython.com/python-keras-text-classification/

# Get a list of dataframe after reading all csv files given a path
def get_all_image_files_read(path, filenames):
    list_img = []
    for file in filenames:
        img = cv2.imread(path+file)
        list_img.append(img)
    return list_img

"""# Deepfake
## Set the Deepfake Working Directory
Finally, we decide to use only one image in a single video
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/American_University/2021_Fall/DATA-793-001_Data Science Practicum/data/Deepfake_facial_extractions
#!pwd

# initial directory 
cwd = os.getcwd()
path_org = './Deepfake_facial_extractions' # the directory where all the output result are located.
extension = 'jpg'

filename = get_list_filenames(cwd, path_org, extension)

print(filename[3393])
print(len(filename))

"""## Save Deepfake features in an array"""

fake = []
for i in range(len(filename)):
  # add constraint
  #if i % 4 == 0:
  fake.append(cv2.imread(filename[i]))
  print("Saving Img {} in an array".format(str(i)))
fake = np.array(fake, dtype=object)
print(fake.shape)

#print(type(fake))
#print(fake.shape)
#plt.imshow(fake[995])
# np.save("fake_3d", fake)

"""## Flatten $X_{fake}$"""

# flatten X
X_fake = []
for i in range(len(fake)):
  X_fake.append(np.ndarray.flatten(fake[i]))
X_fake = np.array(X_fake)
print(X_fake.shape)
#print(type(X_fake))

# Label y
#y = np.zeros(shape=(10, 1))
#print(y.shape)

"""# Label "fake" as $Y$ features"""

np_data = []
for i in range(len(X_fake)):
  np_data.append((X_fake[i], "fake"))
print(np_data)

"""X = []
y = []
for i in data:
  X.append(i[0])
  y.append(i[1])
print(len(X))
print(len(y))

# Real
## Set the Real Working Directory
Finally, we decide to use only one image in a single video
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/American_University/2021_Fall/DATA-793-001_Data Science Practicum/data/Real_facial_extractions
#!pwd

# initial directory 
cwd = os.getcwd()
path_org = '../Real_facial_extractions' # the directory where all the output result are located.
extension = 'jpg'

filename = get_list_filenames(cwd, path_org, extension)

print(filename[6999])
print(len(filename))

"""## Save Real features in an array"""

real = []
for i in range(len(filename)):
  # add constraint
  #if i % 4 == 0:
  real.append(cv2.imread(filename[i]))
  print("Saving Img {} in an array".format(str(i)))
real = np.array(real, dtype=object)
print(real.shape)

#print(type(real))
#print(real.shape)
#plt.imshow(real[999])
# np.save("fake_3d", fake)

"""## Flatten $X_{real}$"""

# flatten X
X_real = []
for i in range(len(real)):
  X_real.append(np.ndarray.flatten(real[i]))
X_real = np.array(X_real)
print(X_real.shape)
#print(type(X_fake))

# Label y
#y = np.zeros(shape=(len(filename), 1))
#print(y.shape)

"""# Label "real" as $Y$ features"""

for i in range(len(X_real)):
  # use the data [] that we have saved the result from "fake" session
  np_data.append((X_real[i], "real"))
print(np_data[-1])

"""# Save the Data"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/American_University/2021_Fall/DATA-793-001_Data Science Practicum/data
#!pwd
np_data = np.array(np_data, dtype=object)
#np.savez_compressed("np_data.npz", np_data)
np.save("np_data_all.npy", np_data)

"""# Define the $X$ and $y$"""

X = []
y = []
for i in np_data:
  X.append(i[0])
  y.append(i[1])
print(len(X))
print(len(y))
print("The length should be " + str((6984+7000)))

"""# Machine Learning Task

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler # standardize features by removing the mean and scaling to unit variance.
from sklearn.metrics import confusion_matrix

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
clf = make_pipeline(StandardScaler(), SVC(gamma='auto')) #clf = classifer
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
confusion_matrix(y_test, y_pred)

from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)

# Testing Zone

# Program to concatenate two 2D arrays column-wise
# import numpy
import numpy as np
 
# Creating two 2D arrays
arr1 = np.arange(1,10).reshape(3,3)
arr2 = np.arange(10,19).reshape(3,3)
arr1
arr2
 
# Concatenating operation
# axis = 1 implies that it is being done column-wise
np.concatenate((arr1,arr2),axis=1)

import numpy as np
  
# creating a numpy array
array = np.array([['a', 'b', 'c'],
                  ['d', 'e', 'f'],
                  ['g', 'h', 'i']])
  
# convert nympy array to dictionary
d = dict(enumerate(array.flatten(), 1))
  
# print numpy array
print(array)
print(type(array))
  
# print dictionary
print(d)
print(type(d))

# References
- https://www.geeksforgeeks.org/python-os-chdir-method/
- https://stackoverflow.com/questions/9234560/find-all-csv-files-in-a-directory-using-python/12280052 
- https://machinelearningmastery.com/how-to-save-a-numpy-array-to-file-for-machine-learning/
- https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch05.html#idm45022165153592
"""