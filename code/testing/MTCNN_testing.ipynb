{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MTCNN_testing.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1qJiOBGVGvywGHMMSLtMcmGypR38jLwqD","authorship_tag":"ABX9TyO0KJ+LRcaWLiv7ZOrgljV8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"on8e6N01drnc"},"source":["#from google.colab import files\n","#uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SSTZ5EwlgS7n"},"source":["image_path = \"/content/drive/MyDrive/American_University/2021_Fall/DATA-793-001_Data Science Practicum/data/data_fake/Deepfake_video1_1.jpg\"\n","! pip install mtcnn\n","from mtcnn import MTCNN\n","import cv2\n","import imageio\n","import matplotlib.pyplot as plt\n","import matplotlib\n","\n","import imageio\n","img = imageio.imread(image_path)\n","#img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n","#img = img[:,:,0] + img[:,:,1]\n","print(img.shape)\n","plt.imshow(img)\n","detector = MTCNN()\n","detections = detector.detect_faces(img)\n","detections"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNB1ixnXrdC2"},"source":["# face detection with mtcnn on a photograph\n","from matplotlib import pyplot\n","from matplotlib.patches import Rectangle\n","from mtcnn.mtcnn import MTCNN\n","\n","# draw an image with detected objects\n","def draw_image_with_boxes(filename, result_list):\n","\t# load the image\n","\tdata = pyplot.imread(filename)\n","\t# plot the image\n","\tpyplot.imshow(data)\n","\t# get the context for drawing boxes\n","\tax = pyplot.gca()\n","\t# plot each box\n","\tfor result in result_list:\n","\t\t# get coordinates\n","\t\tx, y, width, height = result['box']\n","\t\t# create the shape\n","\t\trect = Rectangle((x, y), width, height, fill=False, color='red')\n","\t\t# draw the box\n","\t\tax.add_patch(rect)\n","\t# show the plot\n","\tpyplot.show()\n"," \n","filename = \"/content/drive/MyDrive/American_University/2021_Fall/DATA-793-001_Data Science Practicum/data/data_fake/Deepfake_video1_1.jpg\"\n","# load image from file\n","pixels = pyplot.imread(filename)\n","# create the detector, using default weights\n","detector = MTCNN()\n","# detect faces in the image\n","faces = detector.detect_faces(pixels)\n","# display faces on the original image\n","draw_image_with_boxes(filename, faces)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9Ty5vBzu0zK"},"source":["from PIL import Image\n","import numpy as np\n","\n","def extract_face(filename, required_size=(224, 224)):\n","\t# load image from file\n","\tpixels = pyplot.imread(filename)\n","\t# create the detector, using default weights\n","\tdetector = MTCNN()\n","\t# detect faces in the image\n","\tresults = detector.detect_faces(pixels)\n","\t# extract the bounding box from the first face\n","\tx1, y1, width, height = results[0]['box']\n","\tx2, y2 = x1 + width, y1 + height\n","\t# extract the face\n","\tface = pixels[y1:y2, x1:x2]\n","\t# resize pixels to the model size\n","\timage = Image.fromarray(face)\n","\timage = image.resize(required_size)\n","\tface_array = np.asarray(image)\n","\treturn face_array\n","a = extract_face(\"/content/drive/MyDrive/American_University/2021_Fall/DATA-793-001_Data Science Practicum/data/data_real/real_video752_4.jpg\")\n","print(a.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GlM7C24ivYaS"},"source":["plt.imshow(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OLl0bDG3ESC_"},"source":["\"\"\"\n","detections[0].items()\n","img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n","img = img[:,:,0] + img[:,:,1]\n","img2 = img[185:,:] + img[:,60:]\n","plt.imshow(img2)\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9xAgeXueip_"},"source":["img_with_dets = img.copy()\n","min_conf = 0.9\n","for det in detections:\n","    if det['confidence'] >= min_conf:\n","        x, y, width, height = det['box']\n","        keypoints = det['keypoints']\n","        cv2.rectangle(img_with_dets, (x,y), (x+width,y+height), (0,155,255), 2)\n","        cv2.circle(img_with_dets, (keypoints['left_eye']), 2, (0,155,255), 2)\n","        cv2.circle(img_with_dets, (keypoints['right_eye']), 2, (0,155,255), 2)\n","        cv2.circle(img_with_dets, (keypoints['nose']), 2, (0,155,255), 2)\n","        cv2.circle(img_with_dets, (keypoints['mouth_left']), 2, (0,155,255), 2)\n","        cv2.circle(img_with_dets, (keypoints['mouth_right']), 2, (0,155,255), 2)\n","plt.figure(figsize = (10,10))\n","plt.imshow(img_with_dets)\n","plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p5vKlymLlsA0"},"source":["# Deepfake Images Testing"]},{"cell_type":"code","metadata":{"id":"7xeI3mcDlrtM"},"source":["image_path = '/content/Deepfake_video25_6.jpg'\n","! pip install mtcnn\n","from mtcnn import MTCNN\n","import cv2\n","import imageio\n","import matplotlib.pyplot as plt\n","img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n","#img = img[:,:,0] + img[:,:,1]\n","print(img.shape)\n","detector = MTCNN()\n","detections = detector.detect_faces(img)\n","detections\n","plt.imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FjrHzmp-l0wC"},"source":["image_path = '/content/Deepfake_video25_6.jpg'\n","img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n","print(img.shape)\n","plt.imshow(img)\n","detector = MTCNN()\n","detections = detector.detect_faces(img)\n","detections"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8P_bvL0kmNu-"},"source":["img_with_dets = img.copy()\n","min_conf = 0.9\n","for det in detections:\n","    if det['confidence'] >= min_conf:\n","        x, y, width, height = det['box']\n","        keypoints = det['keypoints']\n","        cv2.rectangle(img_with_dets, (x,y), (x+width,y+height), (0,155,255), 2)\n","        cv2.circle(img_with_dets, (keypoints['left_eye']), 2, (0,155,255), 2)\n","        cv2.circle(img_with_dets, (keypoints['right_eye']), 2, (0,155,255), 2)\n","        cv2.circle(img_with_dets, (keypoints['nose']), 2, (0,155,255), 2)\n","        cv2.circle(img_with_dets, (keypoints['mouth_left']), 2, (0,155,255), 2)\n","        cv2.circle(img_with_dets, (keypoints['mouth_right']), 2, (0,155,255), 2)\n","plt.figure(figsize = (10,10))\n","plt.imshow(img_with_dets)\n","plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nyQdO5TLaRSn"},"source":["detections"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xPABnyYRYhRI"},"source":["#img = img[:,:,0] + img[:,:,1]\n","img2 = img[243:243+104, 83:83+141]\n","print(img2.shape)\n","plt.imshow(img2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4DWe5bHNWh2q"},"source":["# No need to clone the repo, but in case someone wants to inspect the code.\n","!git clone https://github.com/ipazc/mtcnn.git\n","!git clone https://github.com/ageitgey/face_recognition.git  \n","!apt-get install build-essential cmake\n","!apt-get install libopenblas-dev liblapack-dev \n","!pip3 install dlib\n","!pip3 install face_recognition\n","!pip3 install mtcnn\n","!pip3 install opencv-contrib-python\n","# !ls -la face_recognition/tests/test_images/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRxxhnwUYBAd"},"source":["face_locations = detector.detect_faces(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y3gVx8vatwGH"},"source":["%matplotlib inline\n","import face_recognition\n","from matplotlib import pyplot as plt\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.figsize'] = [16, 10]\n","\n","face_locations = face_recognition.face_locations(img)\n","\n","print(\"Found {} face(s) in this photograph.\".format(len(face_locations)))\n","\n","for face_location in face_locations:\n","\n","    # Print the location of each face in this image\n","    top, right, bottom, left = face_location\n","    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n","\n","    # You can access the actual face itself like this:\n","    face_image = img[top:bottom, left:right]\n","    plt.imshow(face_image)\n","    plt.show()\n","print(face_image.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x1hqIL8IdfcE"},"source":["# References:\n","- https://github.com/ipazc/mtcnn/blob/master/example.ipynb\n","- https://github.com/ipazc/mtcnn\n","- https://machinelearningmastery.com/how-to-perform-face-detection-with-classical-and-deep-learning-methods-in-python-with-keras/\n","- https://machinelearningmastery.com/how-to-perform-face-recognition-with-vggface2-convolutional-neural-network-in-keras/\n"]}]}